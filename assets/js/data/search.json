[ { "title": "Entendendo as Câmeras - O Modelo Pinhole", "url": "/posts/modelo-pinhole/", "categories": "camera, fotogrametria", "tags": "cameras, fotogrametria", "date": "2024-08-28 10:00:00 -0300", "snippet": "Neste artigo falarei sobre o Modelo Pinhonle, um conceito fundamental para entender como relacionamos objetos no mundo real com sua representação no em uma imagem. Figura 1: Modelo Pinhole Fonte: Adaptado de Hartley and Zisserman, p. 154 Na Figura 1, relacionamos um objeto do mundo real (X) à sua representação de plano de imagem (x) usando um sistema de coordenadas (Coordenadas Homogêneas). O ponto C é o Centro de Perspectiva da Lente (PC), e a linha que conecta X, x e C representa um raio de luz, conforme discutido no artigo anterior da Camera Obscura.A Figura 1 mostra ainda um sistema de coordenadas mundial (X, Y, Z) com sua origem posicionada no centro de perspectiva da câmera (PC). O plano da imagem tem seu próprio sistema de coordenadas com o ponto principal (PP) como a origem (representado na imagem por p). Um ponto de um objeto X no mundo corresponde ao ponto x no plano da imagem. A distância focal (f) é mostrada no lado direito.Conhecendo todos os elementos representados na Figura 1, podemos agora começar a matemática. Primeiramente, por semelhança de triângulos, podemos afirmar:\\[\\frac{y}{f} = \\frac{Y}{Z}, \\quad \\frac{x}{f} = \\frac{X}{Z}\\]Se isolarmos y, teremos:\\[y = f \\frac{Y}{Z} , \\quad x = f \\frac{X}{Z}\\]Este é o modelo pinhole com condições simplificadas (lembre-se de que nossa câmera está posicionada na origem do nosso sistema de coordenadas mundial).Conhecendo as coordenadas mundiais de um objeto (X, Y, Z), podemos calcular suas coordenadas no plano de imagem (x, y) usando as equações. Por outro lado, com (x, y) e um valor Z conhecido, podemos calcular (X, Y).Ainda podemos representar estas equações na forma matricial:\\[\\begin{bmatrix}x\\\\y\\\\\\end{bmatrix} = \\begin{bmatrix}\\frac{f}{Z} &amp;amp; 0 &amp;amp; 0 \\\\0 &amp;amp; \\frac{f}{Z} &amp;amp; 0 \\\\0 &amp;amp; 0 &amp;amp; 0\\end{bmatrix} \\begin{bmatrix}X \\\\Y \\\\Z \\\\\\end{bmatrix}\\]Relacionar um ponto do plano da imagem (x, y) à sua posição no mundo real (X, Y, Z) requer tanto a distância focal (f) quanto a profundidade do objeto (Z). Em nosso caso simplificado, Z é simplesmente a distância entre o CP da câmera (na origem do mundo) e o objeto.Agora podemos reescrever esta equação usando um sistema de coordenadas homogêneo para representar nossas coordenadas. Assim, a equação se torna:\\[\\begin{bmatrix}x\\\\y\\\\1\\\\\\end{bmatrix} = \\begin{bmatrix}f&amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\\\0 &amp;amp; f &amp;amp; 0 &amp;amp; 0\\\\0 &amp;amp; 0 &amp;amp;1 &amp;amp; 0\\end{bmatrix} \\begin{bmatrix}X \\\\Y \\\\Z \\\\1\\end{bmatrix}\\]Mas o que acontece se a camera não estiver posicionada na origem do sistema de coordenadas mundial?Para relacionar objetos no mundo real à sua representação no plano da imagem, precisamos saber a posição da câmera (no sistema de coordenadas mundial). Essa posição é definida por dois componentes: Translação: Um vetor 3D (T) que representa a distância da câmera até a origem do sistema de coordenadas mundial; Rotação: Uma matriz de rotação 3D (R) especificando a orientação da câmera no espaço. Esta matriz é composta pelos senos e cossenos dos angulos de rotação entre a camera e o sistema de coordenadas mundiais. \\[T = \\begin{bmatrix}T_x \\\\T_y \\\\T_z\\end{bmatrix}, \\quad R = \\begin{bmatrix}r_{11} &amp;amp; r_{12} &amp;amp; r_{13}\\\\r_{21} &amp;amp; r_{22} &amp;amp; r_{23}\\\\r_{31} &amp;amp; r_{32} &amp;amp; r_{33}\\end{bmatrix}\\]Então, podemos aplicar a rotação e a translação às coordenadas do ponto X no mundo e obtemos:\\[\\begin{bmatrix}x\\\\y\\\\\\end{bmatrix} = \\begin{bmatrix}r_{11} &amp;amp; r_{12} &amp;amp; r_{13}\\\\r_{21} &amp;amp; r_{22} &amp;amp; r_{23}\\\\r_{31} &amp;amp; r_{32} &amp;amp; r_{33}\\end{bmatrix} \\begin{bmatrix}X \\\\Y \\\\Z \\\\\\end{bmatrix} + \\begin{bmatrix}T_x \\\\T_y \\\\T_z \\\\\\end{bmatrix}\\]Combinando a equação anterior com a última da seção anterior usando a forma de matriz estendida e usando coordenadas homogêneas, obtemos:\\[\\begin{bmatrix}x\\\\y\\\\1\\end{bmatrix} = \\begin{bmatrix}f&amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\\\0 &amp;amp; f &amp;amp; 0 &amp;amp; 0\\\\0 &amp;amp; 0 &amp;amp;1 &amp;amp; 0\\end{bmatrix}\\begin{bmatrix}r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; | &amp;amp; T_x\\\\r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; | &amp;amp; T_y\\\\r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; | &amp;amp; T_z \\\\0 &amp;amp; 0 &amp;amp; 0 &amp;amp; | &amp;amp; 1\\end{bmatrix} \\begin{bmatrix}X \\\\Y \\\\Z \\\\1\\end{bmatrix}\\]Este é o modelo pinhole se desconsiderarmos as distorções na imagem. Em Visão Computacional, a primeira matriz (a matriz que contém a distância focal) é conhecida como matriz de calibração (K).ConclusãoAinda não exploramos todas as propriedades do modelo pinhole, mas acho que podemos incluir as distorções da imagem equações em um próximo artigo. Por enquanto, exploramos o modelo pinhole e aprendemos como podemos relacionar as coordenadas de uma representação de objeto em uma imagem com suas coordenadas do mundo real. Vejo vocês no próximo artigo!ReferenciasMultiple View Geometry in Computer Vision, 2nd edition, Richard Hartley and Andrew Zisserman.Avaliação do desempenho da Técnica Structure from Motion para mapeamento de corredores. Natalia C AmorimNatalia Amorim" }, { "title": "Entendendo as Câmeras - A Camera Escura", "url": "/posts/camera-obscura/", "categories": "camera, fotogrametria", "tags": "cameras, fotogrametria", "date": "2024-01-12 10:00:00 -0300", "snippet": "Na Visão Computacional trabalhamos com diversas técnicas para processamento, identificação de padrões e extração dos mais diversos tipos de informações de imagens, porém, muitas vezes, acabamos desconsiderando a importância do instrumento fundamental para o nosso trabalho: A Câmera.Nesta série de artigos quero trazer os principais aspectos das câmeras para que possamos entender melhor seu funcionamento e sejamos capazes de extrair o máximo de informação que elas podem nos oferecer. Os assuntos abordados serão: Fundamentos da Câmera O modelo Pinhole As partes da Câmera Sensores (CCD e CMOS)Ao final da leitura destes artigos, você terá um melhor entendimento do que são, como funcionam e como você pode utilizar as câmeras da melhor forma possível em seus projetos.Vamos ao que importa!A Câmera EscuraAs câmeras são construídas aplicando-se conceitos e modelos fundamentais explorados há muito tempo por diversos pesquisadores. Para entender o funcionamento básico das câmeras, precisamos falar sobre A Câmera Escura.A Câmera Escura foi um experimento realizado utilizando um paralelepípedo oco com paredes internas totalmente pretas, exceto pela parede oposta a um pequeno furo (A) por onde passam os raios de luz e estes projetam-se na parede que não é escura. A Figura 1 ilustra este experimento. Figura 1: Camera Escura Fonte: Adaptado de Andrade (2003) Na Figura 1 temos a representação de um paralelepípedo com uma pequena abertura A por onde os raios de luz (representados pelas finas linhas pretas que se cruzam em A) refletidos por um objeto (a seta), entram e incidem sobre a parede oposta a A, formando uma imagem invertida do objeto em questão. Esta imagem pode ser observada dentro da camera escura quando a parede do fundo é branca ou translúcida.Da Figura 1 podemos perceber alguns relacionamentos matemáticos que podem nos ajudar a entender como os objetos imageados no mundo real se relacionam com suas representações em uma imagem.Podemos definir que a escala de representação do objeto (a seta) é:\\[E = \\frac{p}{P}\\]Se você já tem algum conhecimento sobre as câmeras, já deve imaginar a quem d e D são equivalentes em uma câmera fotográfica. Mas se ainda não faz ideia, não se preocupe, continuaremos a explorar as câmeras e suas propriedades.Agora que entendemos o modelo mais fundamental de câmera, podemos começar a aproximá-lo das câmeras fotográficas que conhecemos.Poderíamos inicialmente substituir a parede oposta ao orifício A por um filme fotográfico (coisa antiga, né?), que receberia os feixes de luz que passam por A e registrariam uma imagem. Porém, ainda existe um problema: A imagem projetada seria fraca, pois recebemos uma pequena quantidade de luz, fazendo com que seja necessário um longo tempo de captura para criar esta imagem.Como podemos resolver este problema? Existe uma forma de registrar esta imagem mais rápido? A resposta é sim! Para isso usamos as lentes! No nosso caso aqui, utilizamos lentes convergentes, pois estas recebem os raios de luz e fazem com que estes convergem para um ponto. A Figura 2 ilustra a Câmera Escura agora com uma lente inserida na abertura A. Figura 2: Câmera Fotográfica Fonte: Adaptado de Andrade (2003) Agora, com a inserção da lente convergente na abertura A, podemos concentrar um número maior de feixes luminosos na parede do fundo da nossa câmera escura, tornando a imagem construída muito mais luminosa. A propósito, vamos chamar esta parede de plano focal, que é basicamente onde os raios luminosos incidem para formar a imagem do objeto em questão.Agora você pode estar se questionando: O que acontece se variarmos a distância d mantendo a mesma lente? Se d aumenta,nossa imagem fica maior (alteração de escala), os feixes de luz estarão muito mais espalhados, fazendo com que nossa imagem fique novamente fraca, pouco luminosa, exigindo que deixássemos o nosso filme fotográfico muito mais tempo exposto à luz. Já podemos perceber que d tem uma relação muito importante tanto com a escala de representação dos objetos na imagem, quanto com o tempo de exposição do filme à luz.A Lei de Gauss para LentesVimos que a adoção de uma lente convergente nos traz benefícios e que existe um relacionamento entre d, o tempo de exposição e o quão brilhosa nossa imagem será se variamos estes parâmetros.Agora, como saber qual d me dá a melhor imagem possível? Para isso utilizando a Lei de Gauss para lentes, que é definida como:\\[\\frac{1}{f} = \\frac{1}{D} + \\frac{1}{d}\\]Note que a fórmula acima contém os seguintes parâmetros: f que é a distância focal de uma lente, também conhecida comprimento focal; d que é a distância entre o centro óptico da lente e a parede de fundo da câmera escura; D que é a distância entre o objeto imageado e o centro óptico da lente.Vamos agora imaginar que a distância D é muito maior que d. Isto faz com que 1/D tenha um valor muito pequeno, sendo possível desprezar este termo, fazendo com que a equação se torne:\\[\\frac{1}{f} = \\frac{1}{d}\\]Logo, vemos que, quando o objeto está muito distânte da câmera em comparação com a distância entre a lente e o plano de projeção, d é igual a distância focal. Lembra que anteriormente falamos que se você ainda não tinha entendido a quem d e D são equivalentes, nós iríamos esclarecer? Pois é, por este motivo, neste caso, d é igual a distância focal.ConclusãoNeste artigo abordamos o modelo mais simples de câmera, entendemos o motivo pelo qual usamos as lentes e também conhecemos alguns dos principais parâmetros que relacionam um objeto imageado com sua projeção em uma imagem. No próximo artigo exploraremos ainda mais este relacionamento através do Modelo Pinhole. Nos vemos lá!Com muito carinho e um pouco de agressão…Natalia AmorimEngenheira em Visão Computacional e fundadora do Grupo OpenCVismo Brasil.ReferênciasAndrade, J. B., 2003. Bittencourt de Andrade. Fotogrametria. Curitiba.Thomas A. Stoffregen tas@umn.edu (2013) On the Physical Origins of Inverted Optic Images, Ecological Psychology, 25:4, 369-382, DOI: 10.1080/10407413.2013.839896https://blog.scienceandmediamuseum.org.uk/introduction-camera-obscura/" }, { "title": "Entendendo o Field of View (FoV)", "url": "/posts/o-que-e-fov/", "categories": "kinect, openni", "tags": "cameras, fov", "date": "2023-07-17 10:00:00 -0300", "snippet": "Neste artigo quero trazer uma explicação objetiva do FoV e também mostrar como você pode facilmente utilizar este parâmetro para tarefas como especificação e posicionamento de cameras.O que é o FoV?Se tratando de câmeras, o FoV é definido como a área visível que pode ser imageada por um sensor. Em outras palavras, o FoV descreve, de certa forma, o quanto o sensor consegue alcançar dadas as configurações da nossa câmera.Mas para conseguirmos calcular este parâmetro, devemos entender geometricamente a sua relação com os demais parâmetros de uma câmera. A Figura 1 mostra de forma bem simplificada uma camera com seu sensor, o sistema de lentes (lens), bem como o Field Dimension (que podemos aqui considerar como uma representação do FoV). Esta representação é conhecida como “The thin lens model”. Figura 1: Thin Lens model Fonte: https://www.scantips.com/lights/fieldofviewmath.html Antes de cairmos de cabeça na matemática, devemos ter em mente que o FoV pode ser representado através de uma medida angular, ou mesmo medidas lineares como horizontal, vertical ou diagonal. O que determina qual destas formas de representar o FoV é a melhor, são as necessidades do seu projeto.Na Figura 1 temos, do lado direito em relação às lentes, o plano do sensor e a distância deste plano até o centro óptico das lentes, a distância focal, representada como f. Ao lado esquerdo das lentes temos a plano “observável” pelo sensor e a distância entre o centro optico das lentes até este plano é d. A soma de d e f é S.Além das grandezas já citadas, temos um ângulo alfa que é formado pela linha que parte do centro optico das lentes e é ortogonal aos planos do sensor e o plano da área observável. Este ângulo é muito importante para calcularmos o FoV.Utilizando trigonometria podemos dizer que:\\[tan(\\alpha) = \\frac{\\frac{sensorSize}{2}}{f}\\]Multiplicando a primeira fração pelo inverso da segunda, temos:\\[tan(\\alpha) = (\\frac{sensorSize}{2f})\\]Agora podemos isolar \\(\\alpha\\):\\[\\alpha =tan^{-1} (\\frac{sensorSize}{2f})\\]Porém, observando a Figura 1, percebemos que:\\[FoV = 2\\alpha\\]\\[FoV_{angular} = 2 tan^{-1}(\\frac{sensorSize}{2f})\\]Assim acabamos de deduzir a fórmula matemática para calcularmos o Field of View de uma câmera. Utilizando valores de sensorSize e distância focal na mesma unidade de medida, você obtém o valor angular do FoV em radianos. Depois basta converter para graus (se for necessário).Vale ressaltar que na literatura, muitas vezes, podemos encontrar o nome AoV (Angle of View) que detona justamente o que chamei aqui de FoV Angular.Como calcular o FoV de uma câmera?Agora que sabemos o que é o FoV e como calculá-lo, é hora de trazermos um exemplo prático para fixar o entendimento deste assunto.Das equações apresentadas, vimos que, para calcular o FoV, temos que conhecer com uma certa confiança dois parâmetros: A distância focal da lente e o tamanho físico do sensor. Perceba que estou falando do tamanho físico do sensor pelo fato de saber sua resolução não ser suficiente. Por exemplo, saber que um sensor tem 1920 x 1080 pixels não nos dá a informação que precisamos, que é saber suas dimensões físicas, por exemplo, um sensor de 1920 x 1080 pixels, em que cada pixel mede 2,1 x 2,1 micrômetros tem 4,032 mm x 2,269 mm (esta última medida é o tamanho físico do sensor).Então vamos imaginar uma situação onde eu tenho uma câmera que possui um sensor como o descrito acima e distância focal de 4 mm. Qual o FoV da minha câmera?De posse da fórmula que deduzimos, podemos calcular o FoV horizontal e o FoV vertical. Então fazemos:\\[FoV Horizontal= 2 tan^{-1}(\\frac{4,032 mm}{2 * 4 mm})\\]\\[FoV Horizontal = 2 tan^{-1}(0,504)\\]\\[FoV Horizontal = 0,933684972 rad =53,50°\\]Agora aplicamos a mesma fórmula para o FoV vertical, usando a dimensão vertical do sensor:\\[FoV Vertical = 2 tan^{-1}(\\frac{2,269 mm}{2 * 4 mm})\\]\\[FoV Vertical = 2 tan^{-1}(0,283625)\\]\\[FoV Vertical = 0,552733982 rad = 31,67°\\]Ok, mas como posso saber as dimensões do FoV?Se você é como eu, provavelmente saber o FoV em medida angular não é suficiente. Queremos mesmo é saber quantos centímetros vamos conseguir imagear com nossa câmera, não é mesmo?Pois bem, podemos calcular o FoV em medidas lineares usando fórmulas deduzidas ainda da Figura 1. Primeiramente, vamos relembrar que o angulo alfa aparece tanto no lado esquerdo, quanto direito das lentes (são ângulos opostos e possuem o mesmo valor), assim podemos dizer que, para o lado direito das lentes:\\[tan(\\alpha) = \\frac{\\frac{sensorSize}{2}}{f}\\]e para o lado esquerdo, temos que:\\[tan(\\alpha) = \\frac{\\frac{FoV}{2}}{d}\\]Como conhecemos o valor de alfa e o valor de d, podemos calcular nosso FoV linear manipulando a equação acima:\\[tan(\\alpha) = \\frac{FoV}{2d}\\]\\[tan(\\alpha) 2d = FoV\\]\\[FoV = 2 tan(\\alpha) d\\]Então, usando os dados do exercício anterior, vamos supor que colocaremos nossa câmera a 1 metro de uma superfície a ser imageada. Podemos calcular os valores do FoV horizontal e vertical como:(Lembrando que dividimos o valor do FoV Angular por 2, pelo fato de que alfa equivale a metade do FoV angular)\\[FoV Horizontal = 2 tan(\\frac{0,933684972}{2}) * 1 metro\\]\\[FoV Horizontal = 1,008 m\\]\\[FoV Vertical = 2 tan({0,55273398}{2}) * 1 metro\\]\\[FoV Vertical = 0,56725 m\\]Em resumoSe você veio aqui só pegar as fórmulas e vazar, tá na mão:\\[FoV_{angular} = 2 tan^{-1}(\\frac{sensorSize}{2f})\\]\\[FoV = 2 tan(\\frac{FOV_{angular}}{2}) d\\]Se você calcular com o tamanho horizontal do sensor, obtem o FoV horizontal. Se calcular inserindo o tamanho vertical do sensor, obtém o FoV vertical.ConclusãoDesta forma ficou bem tranquilo entender e calcular o FoV, não é mesmo? Pensando em facilitar a vida para projetos futuros, criei uma planilha onde você pode calcular o FoV inserindo informações de uma câmera (distância focal e tamanho do sensor) e a distância entre a câmera e o plano a ser imageado. Bem prático e você pode usar sempre que necessário!Acesse a planilha em: aqui!Com muito carinho e um pouco de agressão…Natalia AmorimEngenheira em Visão Computacional e fundadora do Grupo OpenCVismo Brasil.Referênciashttps://www.edmundoptics.com/knowledge-center/application-notes/imaging/understanding-focal-length-and-field-of-view/https://www.scantips.com/lights/fieldofviewmath.htmlhttps://www.scantips.com/lights/fieldofview.html" }, { "title": "Utilizando Kinect v2 no Linux através da OpenCV", "url": "/posts/kinectv2-opencv-openni2/", "categories": "kinect, openni", "tags": "kinect, libfreenect, c++, linux, opencv, pcl, openni", "date": "2022-08-15 10:00:00 -0300", "snippet": "O que é o Kinect?O Kinect é um sensor de movimentos desenvolvido para o Xbox 360 e Xbox One pela Microsoft em conjunto com a empresa Prime Sense. O Dispositivo possúi microfones, cameras RGB, projetores infravermelhos e um sensor CMOS monocromático. Utilizando os projectos e o sensor CMOS é possível estimar mapas de profundidade o que permite recriar a profundidade da cena.O dispositivo foi criado com o intuito permitir uma interface natural para que os usuários interagissem com jogos sem a necessidade de controles. Ele é capaz de detectar a pose de até 4 pessoas simultâneamente.Neste artigo demonstrarei os passos necessários para utilizar o Kinect v2 no linux e como acessá-lo utilizando a opencv.Materiais Kinect v2 Adaptador Kinect v2 para PC PC com linuxDependênciasDurante a escrita deste tutorial, dois sistemas operacionais foram utilizados: Ubuntu 20.04 e Ubuntu 22.04, portanto aqui nesta sessão serão apresentados alguns comandos do gerenciador de pacotes APT que permitirão instalar os pacotes précompilados disponíveis. Para os usuários de outras distribuições estará disponível o link do repositório do github que pode ser usado para compilar as bibliotecas necessárias.OpenNIA OpenNI (Open Natural Interaction) é um projeto Open Source com o objetivo de aprimorar a interoperabilidade de interfaces naturais de usuário, fornecendo um middleware que facilita acesso a dispositivos de interação natural para diferentes aplicações.A PrimeSense encerrou o projeto original OpenNI quando foi comprada pela Apple em novembro de 2013, porém seus antigos parceiros matém um fork da OpenNI 2 como um projeto open source. Repositório: https://github.com/structureio/OpenNI2 Comando APT: apt install libopenni2-dev openni2-utilsOpenCVA OpenCV (Open Computer Vision) é uma biblioteca Open Source que reúne diversos algoritmos de visão computacional. A OpenCV foi construída para prover uma infraestrutura comum para aplicações de visão computacional possuindo suporte para diversas linguagens como: C++, python, java e MATLAB. Repositório: https://github.com/opencv/opencv Comando APT: apt install libopencv-devPara realizar a instalação é recomendado os seguintes passos para garantia que se tenha uma versão da OpenCV com suporte a OpenNI2:git clone https://github.com/opencv/opencv.gitgit clone https://github.com/opencv/opencv_contrib.gitcd opencv_contribgit checkout 4.6.0cd ../opencvgit checkout 4.6.0mkdir buildcd buildcmake -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTING=OFF -DBUILD_PERF_TESTS=OFF -DOPENCV_ENABLE_NONFREE=OFF -DOPENCV_EXTRA_MODULES=../opencv_contrib/modules -DWITH_OPENNI2=ON ..make -j 4sudo make installPCLA PCL (Point Clout Library) é uma biblioteca escrita em C++ especializada no processamento de núvens de pontos. Esta biblioteca possuí algoritmos para filtro, seleção de pontos chave, reconstrução de superfícies, comunicação com sensores, visualização de núvens de pontos e outros tópicos. Repositório: https://github.com/PointCloudLibrary/pcl Comando APT: apt install libpcl-devFazendo funcionarAs bibliotecas descritas na sessão anterior serão nossas interfaces de programação em um nível mais elevado, para que possamos extrair os dados do sensor primeiro é necessário um driver que gerenciará a comunicação entre o sistema operacional e kinect. Como os drivers oficiais do kinect são proprietários e apenas disponíveis na Kinect SDK para o windows, a comunidade inicio a criação de um drive para o linux: o freenect, desenvolvido para a primeira versão do kinect, e freenect2, desenvolvido para a segunda versão do kinect.Neste artigo concentraremos no freenect2 pois estamos trabalhando com o Kinect v2. Nas seguintes sessões compilaremos o projeto LibFreenect2 e configuraremos seu uso.Compilando a libfreenect2git clone https://github.com/OpenKinect/libfreenect2.gitcd libfreenect2git checkout v0.2.1mkdir buildcd buildcmake -DBUILD_OPENNI2_DRIVER=ON -DCMAKE_BUILD_TYPE=Release ..make -j 4sudo make installsudo make install-openni2Liberando acesso ao dispositivo para o usuárioApós compilar o projeto e instalar os artefatos, é necessário liberar o acesso ao dispositivo para os usuários, do contrário os programas que tentarem se comunicar com o kinect precisarão ser executados em modo root.cd libfreenect2sudo cp ./platform/linux/udev/90-kinect2.rules /etc/udev/rules.d/Após a execução dos comandos acima é necessário desconectar o kinect da porta USB e conectar novamente.Testando com NiViewer2Como primeiro teste rápido é possivel utilizar agora o NiViewer2 para obter dados do kinect.sudo apt install openni2-utils # caso não tenha sido instalado aindaNiViewer2Após a execução dos comandos acima são esperados uma tela conforme a image e a seguinte saída no terminal:[Info] [Freenect2Impl] enumerating devices...[Info] [Freenect2Impl] 10 usb devices connected[Info] [Freenect2Impl] found valid Kinect v2 @2:2 with serial 067886733447[Info] [Freenect2Impl] found 1 devices[Info] [Freenect2DeviceImpl] opening...[Info] [Freenect2DeviceImpl] transfer pool sizes rgb: 20*16384 ir: 60*8*33792[Info] [Freenect2DeviceImpl] opened[Info] [Freenect2DeviceImpl] starting...[Info] [Freenect2DeviceImpl] submitting rgb transfers...[Info] [Freenect2DeviceImpl] submitting depth transfers...[Info] [Freenect2DeviceImpl] started[Info] [DepthPacketStreamParser] 32 packets were lost[Info] [DepthPacketStreamParser] 13 packets were lost[Info] [TurboJpegRgbPacketProcessor] avg. time: 9.53263ms -&amp;gt; ~104.903HzPrograma de TesteFeito o teste com o NiView2, a seguir está um programa em C++ utilizando a PCL e OpenCV para obter informação do sensor e exibir a núvem de pontos em uma tela interativa. Ao final está contido o vídeo com o resultado esperado ao se compilar o programa abaixo.Código Fontefind_package(OpenCV 4 COMPONENTS core highgui videoio imgcodecs)if(${OpenCV_FOUND})else() find_package(OpenCV 2 COMPONENTS core highgui REQUIRED)endif()find_package(PCL COMPONENTS visualization REQUIRED)set(OPENNI_CAPTURE_INCLUDE_DIRS ${OpenCV_INCLUDE_DIRS} ${PCL_INCLUDE_DIRS})file(GLOB OPENNI_CAPTURE_SOURCES main.cpp)include_directories(${OPENNI_CAPTURE_INCLUDE_DIRS})add_executable(openni-capture ${OPENNI_CAPTURE_SOURCES})target_link_libraries(openni-capture ${OpenCV_LIBS} ${PCL_LIBRARIES})#include &amp;lt;opencv2/core/core.hpp&amp;gt;#include &amp;lt;opencv2/highgui/highgui.hpp&amp;gt;#include &amp;lt;opencv2/imgproc/imgproc.hpp&amp;gt;#include &amp;lt;iostream&amp;gt;#include &amp;lt;pcl/visualization/cloud_viewer.h&amp;gt;#include &amp;lt;fstream&amp;gt;cv::VideoCapture cap(1, cv::CAP_OPENNI2); // Configura o cv::VideoCapture para abrir o dispositivo 1 utilizando a OpenNI2void getPointCloud(pcl::visualization::PCLVisualizer&amp;amp; viewer){ viewer.removeAllPointClouds(); // Remove as núvens de pontos anteriores cv::Mat pointCloudFrame, frame; cap.grab(); // Obtém-se uma leitura do sensor cap.retrieve(pointCloudFrame, cv::CAP_OPENNI_POINT_CLOUD_MAP); // Obtendo núvem de pontos em formato matricial cap.retrieve(frame, cv::CAP_OPENNI_BGR_IMAGE); // Obtendo informação de cor alinhada com a núvem de pontos pcl::visualization::CloudViewer::ColorCloud* cloud = new pcl::visualization::CloudViewer::ColorCloud(); for(int i = 0; i&amp;lt;pointCloudFrame.rows; i++) // Para cada ponto contido na matriz { for(int j = 0; j&amp;lt;pointCloudFrame.cols; j++) // Para cada ponto contido na matriz { cv::Vec3b color = frame.at&amp;lt;cv::Vec3b&amp;gt;(i,j); // Obtém a informação de cor pcl::PointXYZRGB point(color[2], color[1], color[0]); // Cria um ponto no formato da PCL passando a informação da cor contida nesse ponto cv::Vec3f coords = pointCloudFrame.at&amp;lt;cv::Vec3f&amp;gt;(i,j); // Obtém as coordenadas deste ponto point.x = coords[0]; // Adiciona a coordenada ao ponto point.y = coords[1]; // Adiciona a coordenada ao ponto point.z = coords[2]; // Adiciona a coordenada ao ponto cloud-&amp;gt;push_back(point); // Adiciona o ponto na núvem } } viewer.addPointCloud(pcl::visualization::CloudViewer::ColorCloud::ConstPtr(cloud)); // Envia a núvem de pontos para ser renderizada pelo visualizador}int main(int argc, char** argv){ if(!cap.isOpened()) return -1; cap.set(cv::CAP_OPENNI_DEPTH_GENERATOR_REGISTRATION, 1); // Configurando OpenNI para gerar núvem de pontos com informações de cores pcl::visualization::CloudViewer viewer(&quot;point cloud&quot;); // Inicializa um visualizador de núvens de pontos viewer.runOnVisualizationThread(getPointCloud); // Define a função utilizada para renderizar a cena while(!viewer.wasStopped()); // Aguarda a conclusão do visualizador de núvens de pontos cap.release(); // Encerra a captura do dispositivo return 0;}ResultadoCaso tenha dúvidas me envie um email,Matheus Barcelos de OliveiraEngenheiro de Visão Computacional" }, { "title": "VCPKG, como usar ?", "url": "/posts/VCPKG_como_usar/", "categories": "desenvolvimento de software, gerenciador de pacotes, c++", "tags": "vcpkg, windows, c++, cmake", "date": "2022-03-05 21:18:34 -0300", "snippet": "Antes de tudo, o que é VCPKG ?Te respondo , é um gerenciador de pacotes tal como Pip (Python) ou Maven (Java) para o C++, que carecia de um gerenciador tão poderoso como esse.Criado pela Microsoft, visa centralizar, gerenciar e utilizar os pacotes com Cmake e seu compilador para ser de fácil utilização seus pelos desenvolvedoresTe levarei por cada passo que é importante fazer para que o VCPKG funcione de forma coerente.Aahh ia me esquecendo, também funciona para o Linux.Começando pelo começo, este tutorial está voltado ao Windows.Primeiro instale o Cmake em sua máquina.Link : https://cmake.org/download/ Agora instale o Visual Studio Community, ele é necessário por conter o compilador de C e C++ necessários para compilar as bibliotecas.Nota : Usaremos a versão comunity em inglês nesse tutorial.Ao instalar o Visual Studio, selecione Desenvolvimento para desktop em C++. Usando o Git Bash para baixar Vcpkg atráves do PowerShell (administrador) iremos começar o procedimento de instalação.git clone https://github.com/Microsoft/vcpkg.gitAcesse a pastacd vcpkg/Digite os comandos abaixo:.\\bootstrap-vcpkg.bat.\\vcpkg.exe integrate installAo final da instalação, irá aparecer o seguinte resultado.-DCMAKE_TOOLCHAIN_FILE=D:/andreemidio/libraries/vcpkg/scripts/buildsystems/vcpkg.cmakeEssa informação é importante para quando criar um novo projeto C++ e precisar linkar as bibliotecas, é nesse local que o Cmake irá montar as informações.Mais abaixo irei mostrar como usar essa configuração numa IDE.Adicionando o Auto complete:.\\vcpkg.exe integrate powershellAinda no terminal na pasta do clone do VCPKG, testa o funcionamento instale a lib Curl, que serve para fazer chamadas HTTP..\\vcpkg.exe install curl --debugUsando o toolchain (VCPKG) em uma IDEPara Clion da Jetbrains Para o VSCODE da Microsoft{ &quot;cmake.configureSettings&quot;: { &quot;CMAKE_TOOLCHAIN_FILE&quot;: &quot;D:/andreemidio/libraries/vcpkg/scripts/buildsystems/vcpkg.cmake&quot; //Esse caminho se refere ao local onde instalei o VCPKG }, &quot;cmake.configureOnOpen&quot;: true,} Para o QT Creator (Qtzinho da massa para os intímos).Tem uma pequena alteração na string de configuração, mas é simples, só mudar o caminho da pasta que está feito.A configuração passo a passo está no gif meus bacanas.-DCMAKE_TOOLCHAIN_FILE:STRING=D:/andreemidio/libraries/vcpkg/scripts/buildsystems/vcpkg.cmake Código para testar a lib acimaCMakeLists.txtcmake_minimum_required(VERSION 2.8)project(teste)find_package(CURL CONFIG REQUIRED)add_executable(teste main.cpp)target_link_libraries(teste PRIVATE CURL::libcurl)No arquivo de código :main.cpp#include &amp;lt;iostream&amp;gt;#include &amp;lt;curl/curl.h&amp;gt;static size_t WriteCallback(void *contents, size_t size, size_t nmemb, void *userp){ ((std::string*)userp)-&amp;gt;append((char*)contents, size * nmemb); return size * nmemb;}int main(){ CURL * curl; CURLcode res; std::string readBuffer; curl = curl_easy_init(); if(curl) { curl_easy_setopt(curl, CURLOPT_URL, &quot;http://pudim.com.br/&quot;); curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback); curl_easy_setopt(curl, CURLOPT_WRITEDATA, &amp;amp;readBuffer); res = curl_easy_perform(curl); curl_easy_cleanup(curl); std::cout &amp;lt;&amp;lt; readBuffer &amp;lt;&amp;lt; std::endl; } return 0;}O resultado no terminal será algo que está abaixoÉ o nosso saudoso pudim.com.br&amp;lt;html&amp;gt;&amp;lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&amp;gt;&amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Pudim&amp;lt;/title&amp;gt; &amp;lt;link rel=&quot;stylesheet&quot; href=&quot;estilo.css&quot;&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;div&amp;gt; &amp;lt;div class=&quot;container&quot;&amp;gt; &amp;lt;div class=&quot;image&quot;&amp;gt; &amp;lt;img src=&quot;pudim.jpg&quot; alt=&quot;&quot;&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&quot;email&quot;&amp;gt; &amp;lt;a href=&quot;mailto:pudim@pudim.com.br&quot;&amp;gt;pudim@pudim.com.br&amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;script&amp;gt; (function(i,s,o,g,r,a,m){i[&#39;GoogleAnalyticsObject&#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,&#39;script&#39;,&#39;//www.google-analytics.com/analytics.js&#39;,&#39;ga&#39;); ga(&#39;create&#39;, &#39;UA-28861757-1&#39;, &#39;auto&#39;); ga(&#39;send&#39;, &#39;pageview&#39;);&amp;lt;/script&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;Andre EmidioDesenvolvedor em Visão Computacional/Backend e membro do Grupo OpenCV Brasil." }, { "title": "Aquisição de dados usando o Sensor Kinect do Xbox 360", "url": "/posts/capturando_dados_kinect/", "categories": "kinect, libfreenect", "tags": "kinect, libfreenect, c++, opencv", "date": "2022-02-03 21:14:34 -0300", "snippet": "Neste artigo trago um rápido tutorial para que você possa começar a utilizar o sensor kinect do xbox 360 (Kinect v1) para adquirir imagens RGB e mapa de profundidade. Usaremos a libfreenect (OpenKinect) e a OpenCV em linguagem C++ para aprendermos a obter dados do kinect.1. Instalando as bibliotecasCompilando a OpenCV#dependencias da opencvsudo apt-get updatesudo apt-get install build-essential cmake unzip pkg-configsudo apt-get install libjpeg-dev libpng-dev libtiff-devsudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-devsudo apt-get install libxvidcore-dev libx264-devsudo apt-get install libgtk-3-devsudo apt-get install libatlas-base-dev gfortran libeigen3-devcd ~git clone https://github.com/opencv/opencv.gitgit clone https://github.com/opencv/opencv_contrib.gitcd ~/opencvmkdir build &amp;amp;&amp;amp; cd buildcmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_PYTHON_EXAMPLES=OFF \\ -D INSTALL_C_EXAMPLES=ON \\ -D OPENCV_ENABLE_NONFREE=ON \\ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \\ -D BUILD_EXAMPLES=ON ..make -j4sudo make installCompilando a libfreenectcd ~sudo apt-get install git build-essential libusb-1.0-0-dev cython3 libglfw3-dev#instalando a librealsensesudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE || sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDEsudo add-apt-repository &quot;deb https://librealsense.intel.com/Debian/apt-repo $(lsb_release -cs) main&quot; -usudo apt-get install librealsense2-dkmssudo apt-get install librealsense2-utils# para compilar os exemplossudo apt-get install freeglut3-dev libxmu-dev libxi-devgit clone https://github.com/OpenKinect/libfreenectcd libfreenectmkdir build &amp;amp;&amp;amp; cd buildcmake -DBUILD_PYTHON3=ON -DCYTHON_EXECUTABLE=/usr/bin/cython3 ..#pra caso você queira utilizar também com python#lembre-se que você deve ter o python e a numpy instalados para usar as flags acimamakesudo make installldconfig -v2. Criando o ProjetoPreparando o CMakeListsCrie uma pasta chamada kinect-project em algum lugar de sua preferência. Dentro desta pasta, crie um arquivo chamado main.cpp e um arquivo chamado CMakeLists.txt.A estrutura de seu projeto deve ficar desta forma:--kinect-project |__ main.cpp |__ CMakeLists.txtDentro do arquivo CMakeLists.txt coloque o conteúdo abaixo:cmake_minimum_required(VERSION 2.8 FATAL_ERROR)find_package(OpenCV REQUIRED)find_package(Threads REQUIRED)find_package(libfreenect REQUIRED)include_directories( /usr/local/include/libfreenect/ #verifique se os includes da sua instalação realmente estão neste caminho /usr/local/include/libusb-1.0/ #idem pra esses includes aqui)add_executable( kinect_project main.cpp)target_link_libraries(kinect_project ${OpenCV_LIBS} ${CMAKE_THREAD_LIBS_INIT} ${FREENECT_LIBRARIES} freenect)Escrevendo o CódigoCaso esteja com pressa, o código-fonte completo deste artigo pode ser obtido aqui. (Apesar do módulo se chamar Calibration, o código ainda não faz isso).Inicializando a freenect e acessando o dispositivoA primeira coisa que precisamos fazer é adicionar os includes necessários para o nosso projeto:#include &amp;lt;signal.h&amp;gt;#include &amp;lt;stdbool.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;string.h&amp;gt;#include &quot;libfreenect.h&quot;#include &amp;lt;opencv2/core.hpp&amp;gt;#include &amp;lt;opencv2/highgui.hpp&amp;gt;#include &amp;lt;opencv2/imgproc.hpp&amp;gt;Agora podemos começar a escrever o conteúdo da nossa função main. A primeira coisa a se fazer é inicializar a freenect:int main(int argc, char** argv){ freenect_context* fn_ctx; int ret = freenect_init(&amp;amp;fn_ctx, NULL); if (ret &amp;lt; 0) return ret; //Caso não tenha sucesso em inicializar, para a execução}O próximo passo é encontrar os dispositivos kinect conectados em sua máquina (você conectou o kinect, não é?). Para isso, adicione o seguinte trecho no seu código:int num_devices = ret = freenect_num_devices(fn_ctx);if (ret &amp;lt; 0) return ret;if (num_devices == 0){ std::cout &amp;lt;&amp;lt; &quot;Nenhum dispositivo conectado!&quot; &amp;lt;&amp;lt; std::endl; freenect_shutdown(fn_ctx); return 1;}Depois que encontramos os dispositivos conectados, devemos acessar pelo menos um deles para começarmos a obter os dados. Fazemos isso adicionando o seguinte trecho de código:freenect_device* fn_dev; // Variável que vai guardar as informações do dispositivoret = freenect_open_device(fn_ctx, &amp;amp;fn_dev, 0); //Acessando o primeiro dispositivo que possui indice 0 na listaif (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); // Caso não consiga acessar o dispositivo, encerra a freenect e para a execução return ret;}Agora que já temos acesso ao dispositivo, podemos configurar o modo que receberemos as informações do sensor RGB e do sensor de profundidade:ret = freenect_set_depth_mode(fn_dev, freenect_find_depth_mode(FREENECT_RESOLUTION_MEDIUM, FREENECT_DEPTH_MM));if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}ret = freenect_set_video_mode(fn_dev, freenect_find_video_mode(FREENECT_RESOLUTION_MEDIUM, FREENECT_VIDEO_RGB));if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}Criando as funções de CallbackPara para sermos capazes de manipular os frames que recebemos dos sensores RGB e IR, precisamos definir as funções de callback, que serão chamadas sempre que um novo frame chegar. Então, vamos criar uma função callback para receber o frame RGB e outra callback para receber o frame de profundidade.//função callback para o frame de profundidadevoid depth_cb(freenect_device* dev, void* data, uint32_t timestamp){ printf(&quot;Received depth frame at %d\\n&quot;, timestamp); cv::Mat depthMat(cv::Size(640,480),CV_16UC1); uint16_t* depth = static_cast&amp;lt;uint16_t*&amp;gt;(data); depthMat.data = (uchar*) data; cv::imwrite(&quot;depth.png&quot;, depthMat);}//Função callback para o frame RGBvoid video_cb(freenect_device* dev, void* data, uint32_t timestamp){ printf(&quot;Received video frame at %d\\n&quot;, timestamp); cv::Mat rgbMat(cv::Size(640,480), CV_8UC3, cv::Scalar(0)); uint8_t* rgb = static_cast&amp;lt;uint8_t*&amp;gt;(data); rgbMat.data = rgb; cv::imwrite(&quot;rgb.png&quot;, rgbMat);}Agora, dentro da função main precisamos precisamos “registrar” essas funções callback para que sejam chamadas a cada novo frame:freenect_set_depth_callback(fn_dev, depth_cb); //Lembra que fn_dev é a variável que guarda as informações do dispositivo, né?freenect_set_video_callback(fn_dev, video_cb);Iniciando a captura de framesPara iniciar a captura dos frames, utilizamos a função freenect_start_depth para capturar os frames de profundidade e freenect_start_video para captura dos frames RGB. Copie e cole o trecho abaixo:ret = freenect_start_depth(fn_dev);if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}ret = freenect_start_video(fn_dev);if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}Para manter a captura rodando até que uma interrupção seja requisitada, vamos usar um recurso chamado signals (sinais). A ideia é emitir um sinal toda vez que uma requisição de interrupção do nosso programa for feita. Para isso, vamos declarar uma variável global que armazena o estado do nosso programa e uma função que irá lidar com o sinal (handler). Copie e cole o trecho de código abaixo://variável globalvolatile bool running = true;//função que será chamada quando o sinal de interrupção for emitidovoid signalHandler(int signal){ if (signal == SIGINT || signal == SIGTERM || signal == SIGQUIT) { running = false; }}Agora, dentro da função main colocamos o seguinte loop:while (running &amp;amp;&amp;amp; freenect_process_events(fn_ctx) &amp;gt;= 0){}Depois que o nosso loop for interrompido, devemos parar a execução da freenect de forma apropriada. Então, depois do loop, cole o seguinte trecho de código:std::cout &amp;lt;&amp;lt; &quot;Parando a execução&quot; &amp;lt;&amp;lt; std::endl;// Stop everything and shutdown.freenect_stop_depth(fn_dev);freenect_stop_video(fn_dev);freenect_close_device(fn_dev);freenect_shutdown(fn_ctx);std::cout &amp;lt;&amp;lt; &quot;Encerrado com sucesso!&quot; &amp;lt;&amp;lt; std::endl;return 0;Compilando e Executando o ProjetoAgora podemos finalmente compilar e executar nosso projeto. Para isso, utilizaremos o cmake, então basta executar os seguintes comandos dentro da pasta do nosso projeto:mkdir build &amp;amp;&amp;amp; cd buildcmake ..make./kinect_projectPronto! Agora você verá alguns prints passando no seu terminal (prints que vieram das funções callback) e você pode requisitar a interrupção do programa com um Ctrl+c. Depois que a execução parar, é só olhar dentro da sua pasta build, as imagens rgb.png e depth.png estão salvas lá dentro.Assim você acabou de adquirir uma imagem RGB e uma imagem de profundidade usando o sensor Kinect com a lifreenect e a OpenCV :)Com muito carinho e um pouco de agressão…Natália AmorimEngenheira em Visão Computacional e fundadora do Grupo OpenCV Brasil." }, { "title": "Como contribuir neste Blog", "url": "/posts/como_contribuir/", "categories": "contribuicao, artigos", "tags": "getting started", "date": "2022-01-23 16:42:34 -0300", "snippet": "Aprenda como enviar o seu tutorial em forma de artigo para o Blog oficial do Grupo OpenCV Brasil (apelidado carinhosamente de OpenCVismo Brasil).1. Fork o repositório do BlogAcesse o repositório do nosso blog e dê um fork.2. Escreva seu artigo em um arquivo markdownAntes de escrever o conteúdoAntes de começar a escrever o conteúdo propriamente dito, lembre-se de colocar o cabeçalho que todos os nossos artigos devem ter, um exemplo:---title: O título do seu Artigoauthor: name: Seu Nome link: link para seu github ou linkedindate: 2022-01-23 16:42:34 -0300categories: [categaria 1, categoria2]tags: [tag1 tag2]pin: false--- Escreva seu título entre aspas duplas (sim, é uma string); Escreva seu nome e coloque um link para uma rede social sua: Pode ser seu blog, linkedin ou github. O importante é o leitor te encontrar! Modifique a data para a data em que você escreveu seu artigo, o formato da data é aaaa-mm-dd (ano-mês-dia); Escreva em quais categorias seu conteúdo se encaixa (Ex: filtros deepLearning Yolo). As categorias devem ser separadas por espaço e cada artigo deve ter no máximo três categorias. O mesmo vale para as tags Inserindo ImagensImagens podem ser inseridas no artigo para melhorar o entendimento do leitor. Você pode utilizar a própria sintaxe da linguagem markdown para fazer isso de forma simples e rápida. Um exmeplo:![image info](link-para-a-sua-imagem)Se você produziu imagens para inserir no seu tutorial, crie uma pasta dentro de /assets/img/imagens/nome-da-sua-pasta e coloque as imagens que você produziu dentro do seu diretório. Assim, você pode chamar essas imagens em seu artigo da seguinte forma:![image info]({{ site.baseurl }}/assets/img/imagens/nome-da-sua-pasta/sua-imagem.png)Inserindo código-fonteEste não deveria precisar de maiores explicações, não é? Para inserir seu código fonte, basta usar a sintaxe padrão do markdown para código: Conteúdo do código entre uma par de três crases. Um exemplo:Deixe sua assinatura no artigoNo final do artigo você pode colocar uma assinatura para deixar as pessoas saberem que você é o autor. Olhe como os outros membros fizeram suas assinaturas e construa a sua.Salvando o artigoSeu artigo deve estar em um arquivo markdown (.md) e deve ser colocado dentro do diretório _posts. Fique atento para dar o nome correto ao seu arquivo, perceba que o nome do arquivo .md tem uma padronização, por exemplo:2022-01-23-como_contribuir.md Primeiro insira a data que você criou este conteúdo no formato AAAA-MM-DD (ano-mês-dia); Depois coloque um título resumido do seu artigo (com no máximo três palavras separadas por underline); O nome do seu artigo deve ficar com o mesmo padrão do exemplo acima. 3. Deploy localVocê pode testar o blog localmente em sua máquina, assim você testa se o site está funcionando com seu artigo antes de enviar a pull request e quebrar o site blog. As instruções sobre o que você precisa instalar e como rodar subir o site localmente podem ser encontradas aqui . Não seja preguiçoso(a), leia! :)4. Envie uma Pull RequestAgora que você já escreveu, se atentou para os padrões necessários, conferiu se o site não quebrou com as suas atualizações, é hora de enviar uma PR para qe seu artigo entre para o blog oficial: Abra uma PR e marque Natália Carvalho (NataliaCarvalho03) como revisora; Espere que o processo de revisão termine, caso haja alguma coisa a corrigir, você será avisado. ConclusãoEntão, garotos e garotas, é isso! Sintam-se a vontade para contribuir com novos artigos e qualquer dúvida, basta nos contatar nos grupos do Telegram ou Discord!Com muito carinho e um pouco de agressão…Natália C. de AmorimEngenheira em Visão Computacional e fundadora do Grupo OpenCV Brasil" }, { "title": "Detecção de objetos com haarcascade", "url": "/posts/cap5/", "categories": "detecção de objetos, haarcascade", "tags": "detecção de objetos, classificação, python, opencv", "date": "2020-12-04 08:25:34 -0300", "snippet": "Detecção de objetos usando o método Haar CascadeNesse capítulo você irá aprender uma maneira rápida e direta de como criar um classificador Haar Cascade. Além disso, no final do capítulo será disponibilizado um código para detecção de objetos com o classificador criado, que com pequenas alterações, pode se adequar a qualquer situação.Nesse contexto, o método Haar Cascade, é um método de detecção de objetos proposto por Paul Viola e Michael Jones. É uma abordagem baseada em Machine Learning, em que uma função cascade é treinada com muitas imagens positivas e negativas. Logo, é usado para detectar objetos em outras imagens.Preparando o ambientePara esse projeto é necessário que você tenha instalado em sua máquina apenas 3 itens.1 - Editor de textos de sua preferência, eu particularmente uso o Visual Studio Code.2 - Alguma versão Python de sua preferência, eu particularmente uso a 3.8.5.3 - Biblioteca OpenCV .Aqui não irei explicar como você pode fazer o download e instalação desses itens, pois na internet existem diversos tutorias detalhados de como fazer isso.PassosA criação de um classficador usando o HaarCascade pode ser descrita em um conjunto de 5 passos.1 - Escolher o objeto.2 - Selecionar imagens negativas.3 - Selecionar imagens positivas.4 - Gerar o vetor de positivas.5 - Treinar o classificador.1 - Escolher o objeto.O primeiro passo é escolher o objeto que será identificado, para isso você deverá pensar nos seguintes aspectos:* Serão objetos rígidos como uma logo (nike) ou com variações (cadeira,copo)?* Objetos rigidos são mais eficientes e mais rápidos.* Ao treinar muitas variações pode ser que o classificador fique fraco, portanto, fique atento a isso.* Objetos que a cor é fundamental não são recomendados, pois as imagens serão passadas para a escala de cinza.Para esse projeto, escolhi o objeto faca para ser detectado.2 - Selecionar imagens negativas.Para selecionar as imagens negativas, você deve ficar atento aos seguintes aspectos:* Podem ser qualquer coisa, menos o objeto.* Devem ser maiores que as positivas, pois a openCV vai colocar as imagens positivas dentro das negativas.* Se possível usar fotos de prováveis fundos onde o objeto é encontrado. *Ex: Objeto = carro Usar imagens de asfalto e ruas vazias.Logo, você deve ficar atento as imagens escolhidas, pois como dito elas podem ser qualquer coisa, exceto o objeto escolhido, como escolhemos facas como objeto de detecção devemos, iremos precisar de imagens que não tenham facas.Quantas imagens negativas?É relativo, para esse projeto eu conto com 3000 imagens negativas, com diversas variações de fundo. Entretanto, isso vai depender dos resultados obtidos no treinamento, pode ser que eu precise de mais imagens ou não, isso será explicado mais a frente com mais detalhes.Exemplos de imagens negativas: Figura 1 Figura 2 Figura 3OBS: Todas essas imagens tem dimensões 100x100, essa informação será importante para futuras explicações.Aqui é importante mencionar que você deve criar uma pasta (ex: projeto) onde estará outra pasta com as imagens negativas, na pasta projeto também deve estar as imagens positivas.Exemplo: PastaNa pasta das imagens negativas você deve colocar esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/criar_lista.batE deve executá-lo ao final da escolha das imagens negativas, pois ele vai gerar uma lista com as imagens negativas.3 -Selecionar imagens positivas.Para selecionar as imagens positivas, você deve ficar atento aos seguintes aspectos:* Apenas o objeto.* Quantas imagens? * Depende da: Qualidade da imagem, tipo do objeto, poder computacional disponível.* As imagens devem ter o mesmo tamanho e a proporção precisa ser a mesma, caso contrário a openCV faz isso automaticamente e gera problemas de distorção do objeto. * Ex: Uma imagem 100x50, passada pra 25x25, vai ter o objeto descaracterizado.* Imagens grandes podem gerar problemas, fazendo o treinamento durar até meses.* Sempre que possível usar imagens com fundo branco.Como dito no primeiro passo, você deve tomar cuidado com as variações do objeto, caso você queira realizar a detecção de um objeto em diferentes ângulos é sugerido que você faça diferentes classificadores. Para exemplificar isso, cito o classificador haarcascade frontal face, que realiza a detecção frontal da face (esse classificador inclusive é de fácil acesso, até a openCV disponibiliza ele pra você) e caso você queira a detecção lateral da face, você deve usar outro classificador, esses cuidados devem ser tomados para que você tenha um bom classificador.Em relação a quantidade, alguns estudos sugerem que um bom classificador deve ter no mínimo 5000 mil imagens como entrada para o treinamento.Exemplos de imagens positivas que irei usar para o treinamento do classificador: Figura 1 Figura 2 Figura 3OBS: Todas essas imagens tem dimensões 100x50, essa informação será importante para explicações futuras.Aqui temos o pulo do gato, é possível criar mais imagens positivas a partir das imagens que você já tem, para isso baixe esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/opencv_createsamples.exeColoque ele junto com as imagens positivas, depois basta abrir o CMD, entrar na pasta onde estão suas imagens positivas e digitar esse comando:opencv_createsamples -img faca_1.png -bg negativas/bg.txt -info positivas/positivas.lst -maxxangle 0.5 -maxyangle 0.5 -maxzangle 0.5 -num 300 -bgcolor 255 -bgthresh 10Parâmetros:-img = Nome da imagem base.-bg = Nome da pasta / nome do arquivo .txt com as informações das imagens negativas.-info = Nome da pasta / Nome do arquivo .lst (sempre altere esse parâmetro quando usar uma nova imagem (Ex: positivas2/positivas2.lst, positivas3/positivas3.lst)).-maxangle (x,y,z) = Variação de rotação que a imagem terá.-num = Número de imagens que serão criadas.-bgtresh = parâmetro que permite a retirada do fundo da imagem, deixando apenas o objeto de interesse (aqui se justifica o fundo branco). Esse parâmetro deve ser analisado com cuidado, pois: bgtresh 10 bgtresh 100Como resultado você terá a quantidade de imagens mencionada em uma pasta de acordo com o nome que você escolheu e um arquivo .lst que terá informações sobre essas imagens, esse arquivo é de extrema importância e é a partir dele que iremos criar o vetor de imagens.Nesse caso, usei 10 imagens e criei um total de 3000 imagens a partir desse comando, logo, você terá as pastas positivas1, positivas2 e etc…Ex: Pasta4 - Gerar o vetor de positivas.Aqui você deverá criar um vetor para cada pasta com as imagens positiva (positivas1, positivas2, etc…) e depois juntar esses vetores em apenas um vetor.Para isso, digite esse comando no CMD:opencv_createsamples -info positivas1/positivas1.lst -num 2000 -w 50 -h 25 -vec vetor1.vecParâmetros:-info = Nome da pasta que contém as imagens / arquivo .lst (Como criamos 3000 imagens a partir de 10 imagens, temos 10 pastas com 300 imagens cada, logo, teremos que repetir esse comando 10 vezes, alterando o nome da pasta e do arquivo .lst, para positivas2 / positivas2.lst, etc…). w e -h = são as dimensões, como nossas imagens eram 100 x 50, eu reduzi para 50 x 25, para reduzir o tamanho do arquivo, até porque para treinar o classificador com as imagens em 100 x 50 eu deveria ter um super computador. vec = Nome do vetor (Aqui você também tem que alterar, colocando vetor1, vetor2, etc…). Após isso, devemos unir todos esses vetores em apenas um, para isso crie uma pasta chamada “vec” e coloque todos os vetores nela.Depois baixe esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/mergevec.pye coloque ele na pasta do seu projeto.após isso, digite no CMD:python mergevec.py -v vec/ -o vetor_final.vecApós a conclusão você terá um arquivo chamado vetor_final.vec, que é o vetor que iremos utilizar.Nesse momento a pasta do seu projeto estará assim: Pasta “Projeto”5 - Treinar o classificadorPara o passo final, você deve primeiramente baixar esses arquivos:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/opencv_traincascade.exehttps://mega.nz/file/09YnVKQb#LdE1iz05i9OeoMqoZtuC3lVn4teeA7gqozVS-N1hG2UApós isso, você deve abrir a pasta negativas e colocar esses dois arquivos e o vetor final nela, e criar uma pasta chamada “classificador”.Dessa maneira: Pasta “classificador” ArquivosApós isso, abra o CMD na pasta negativas e digite o seguinte comando:opencv_traincascade -data classificador -vec vetor_final.vec -bg bg.txt -numPos 2400 -numNeg 1200 -numStages 15 -w 30 -h 15 -precalcBufSize 1024 -precalcIdxBufSize 1024Parâmetros-data = Nome da pasta que os arquivos de treinamento serão armazenados.-vec = Nome do vetor.-bg = informações das imagens negativas.-numPos = Número de imagens positivas.-numNeg = Número de imagens negativas.-numStages = Número de estágios.-w e -h = dimensões das imagens.-precalcBufSize e -precalcIdxBufSize = memória utilizada para o treinamento.Após o treinamento, na pasta classificador você terá esses arquivos: ArquivosO arquivo cascade.xml é o nosso classificador.O arquivo params.xml são os parâmetros usados no treinamento.E os outros arquivos, são os resultados de cada estágio do treinamento.Sobre o uso dos parâmetros: É indicado que você use metade do número de imagens positivas para as negativas no primeiro treinamento. Alguns estudos sugerem que um bom classificador deve ter no mínimo 5000 imagens positivas. Após o primeiro treinamento, se você notar que está tendo muitos falsos positivos, aumente o número de imagens negativas, se notar que não está realizando a detecção, aumente o número de imagens positivas, faça novos treinamentos até ter bons resultados. Para melhorar os resultados você também pode aumentar o número de estágios. Não se esqueça que a soma dos parâmetros -precalcBufSize e -precalcIdxBufSize não pode ser maior que a memória disponível. Quanto mais imagens negativas, positivas, estágios de treinamento e dimensão das imagens, mais o treinamento vai demorar, podendo fazer o treinamento durar até meses. Código para detecçãoAgora irei apresentar um código que irá realizar a detecção de facas.Para isso baixe esse classificador que eu criei, ele ainda não está pronto, logo não irá apresentar resultados excelentes.https://github.com/luis131313/cookbook/blob/master/imagens/cap2/cascade_facas.xmlimport cv2#variável que armazena a imagemimagem1 = &#39;teste1.png&#39;#variável que armazena o arquivo xmlcascade_path1 = &#39;cascade_facas.xml&#39; #cria o classificadorclf1 = cv2.CascadeClassifier(cascade_path1)#lê a imagemimg1 = cv2.imread(imagem1)#converte para cinzagray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)#Função da detecçãodeteccoes1 = clf1.detectMultiScale(gray1, scaleFactor=1.01, minNeighbors=5, minSize=(1,1))#desenha o retângulo com as coordenadas obtidasfor(x,y,w,h) in deteccoes1: img1 = cv2.rectangle(img1, (x,y), (x+w, y+h), (0,0,255), 2)#para visualizar a imagemcv2.imshow(&#39;Classificador 1&#39;, img1)#mantém a janela aberta até que eu digite uma teclacv2.waitKey(0)#destrói a janelacv2.destroyAllWindows()Ao executar o código com um exemplo, teremos essa detecção: Imagem retirada do Google Podemos notar alguns falsos positivos, o que indica que seria interessante realizar um novo treinamento com mais imagens negativas.Considerações finaisVários dos arquivos apresentados foram cedidos pela www.iaexpert.academy, agradeço imensamente pela generosidade.Fiz esse tutorial com muito carinho e espero que seja útil para você, a intenção aqui foi realizar um pequeno projeto usando o método HaarCascade, ainda existe muito há aprender sobre esse método, mas a minha intenção é apenas introduzir esse assunto.Desejo bons estudos e bons trabalhos.Atenciosamente,Luis Fernando Santos Ferreira, Aluno do curso de Ciência da Computação na Universidade Federal de Lavras.Linkedin: https://www.linkedin.com/in/luis-ferreira-3b02131a8/" }, { "title": "Filtro de densidade usando Machine Learning e Clusterização", "url": "/posts/cap3/", "categories": "agrupamento, densidade, dbscan", "tags": "agrupamento, python, opencv, sklearn", "date": "2020-12-04 08:25:34 -0300", "snippet": "Filtro de densidade usando Machine Learning e ClusterizaçãoNesse capítulo você irá aprender uma maneira rápida e fácil de utilizar Machine learning para remover ruídos em uma captura de objeto utilizando range de cores.Na captura de imagens utilizando cores, um dos grandes problemas encontrados é a remoção do ruído indesejado por objetos menores ao fundo do cenário.Nesse contexto, iremos utilizar o método DBSCAN da Lib SKLEARN para reorganizar os pixels das cores selecionadas e identificar áreas de baixa densidade.Obs. Os códigos contidos neste capítulo foram desenvolvidos para serem rodados no Google Colab e pode sofrer algumas alterações para rodar fora do Colab (exemplo: a utilização da lib google.colab.patches para visualizar imagens).Preparando o ambientePara esse projeto é necessário que você tenha instalado os seguintes itens em sua máquina. Editor de textos de sua preferência. Python 3.8.5 . Bibliotecas (OpenCV, Collections, Sklearn, Numpy, urllib, matplotlib). Caso queira, pode utilizar o Google Colab que tem o ambiente praticamente pronto.PassosImportar Libsimport numpy as npimport urllibimport cv2from google.colab.patches import cv2_imshowfrom collections import Counterfrom sklearn.cluster import DBSCANfrom sklearn import metricsfrom sklearn.datasets import make_blobsfrom sklearn.preprocessing import StandardScalerimport matplotlib.pyplot as pltDownload da imagemBaixamos a imagem diretamente da internet utilizando a lib Urlib e após isso, utilizamos as libs Numpy e Opencv para converter a imagem para um formato aceito pela lib OpenCV.Obs. Como estamos utilizando o Google Colab, estaremos realizando o download e conversão das imagens diretamente do Imgur.def url_to_image(url): resp = urllib.request.urlopen(url) image = np.asarray(bytearray(resp.read()), dtype=&quot;uint8&quot;) image = cv2.imdecode(image, cv2.IMREAD_COLOR) hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) return hsvurl = f&#39;https://i.imgur.com/PLGlnWj.png&#39;img = url_to_image(url)Seleção do range de corEste é um dos momentos mais demorados e manuais da aplicação, pois teremos que manualmente encontrar o range de cores utilizados no objeto que queremos selecionar.No nosso caso é o Azul.img = url_to_image(url)BLUE_MIN = (110,50,50)BLUE_MAX = (130,255,255)Separação de pixelsNesta parte do código percorremos todos os pixels da imagem, verificamos quais encontram-se dentro do range de cores selecionados e guardamos as coordenadas dos azuis dentro do array data_corddata_cord = []height, width, channels = img.shapefor x in range(height): for y in range(width): r, g, b = img[x,y] if (r,g,b) &amp;gt;= BLUE_MIN and (r,g,b) &amp;lt;= BLUE_MAX: data_cord.append([int(x),int(y)])DBSCANAgora, com o data_cord já separado, iremos utilizar DBSCAN para clusterizar os dados.O DBSCAN é uma algoritmo de machine learning que clusteriza os dados com base em densidade e tamanho de cluster.X = StandardScaler().fit_transform(data_cord)db = DBSCAN(eps=0.1, min_samples=1).fit(X)core_samples_mask = np.zeros_like(db.labels_, dtype=bool)core_samples_mask[db.core_sample_indices_] = Truelabels = db.labels_Apresentação de clusters (Opcional)Este tópico não é obrigatório, mas caso queira visualizar os dados e verificar se está sendo separado corretamente.Utilizando Matplotlib, separamos os labels dos clusters e setamos uma cor para cada cluster.n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)unique_labels = set(labels)colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]for k, col in zip(unique_labels, colors): if k == -1: pass class_member_mask = (labels == k) xy = X[class_member_mask &amp;amp; core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=tuple(col), markeredgecolor=&#39;k&#39;, markersize=14) xy = X[class_member_mask &amp;amp; ~core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=tuple(col), markeredgecolor=&#39;k&#39;, markersize=6)plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_)plt.show()Identificar maior clusterUtilizando o código abaixo, separamos os clusters em dict, verificamos qual o maior e salvamos essa informação em template_max.dict_data = dict(Counter(labels))template_max = max(dict_data, key=dict_data.get)ApresentaçãoNo código abaixo, realizamos a varredura dentro de labels e para cada vez que identificamos o maior cluster, pegamos este index e buscamos a mesma posição no data_cord.Assim, conseguimos pegar as coordenadas que se encontram no maior cluster e podemos manipular as coordenadas do maior cluster, ignorando o ruído geral da imagem.No caso abaixo, estamos pintando de vermelho o maior cluster e pintando de azul os demais(ruídos) apenas para fins demonstrativos.for i in range(len(labels)): if labels[i] == template_max: x,y = data_cord[i] img[x,y] = (0,0,255) else: x,y = data_cord[i] img[x,y] = (255,0,0)cv2_imshow(img)Atenciosamente&amp;lt;/br&amp;gt;Willian Jesus da Silva, Aluno do curso de Ciência da Computação no Instituto de ensino superior da Grande Florianópolis." }, { "title": "Processamento Morfológico de Imagens", "url": "/posts/cap2/", "categories": "morfologia", "tags": "morfologia, python, opencv", "date": "2020-12-04 08:25:34 -0300", "snippet": "Processamento Morfológico de ImagensImagens e ConjuntosEm algumas aplicações de Processamento Digital de Imagens podemos utilizar conceitos da Teoria dos conjuntos para realizarmos algumas análises e inferir certas informações em imagens. Por exemplo, podemos dizer que um objeto, representado em uma imagem, é formado pelo conjunto de pixels que o constituem. Figura 1: Pista de Corrida Na Figura 1 você pode perceber facilmente algumas árvores e uma pista de corrida. Podemos dizer que somos capazes de determinar visualmente as diferenças entre as árvores e a pista de corrida apenas afirmando que o conjunto dos pixels azuis formam a pista de corrida, enquanto o conjunto dos pixels de cor verde-escura formam as árvores.O que eu quero te mostrar neste capítulo é que podemos realizar algumas operações sobre estes conjuntos de pixels que representam objetos nas imagens! A estas operações sobre conjuntos de pixels chamamos de Processamento Morfológico de Imagens.Os Elementos EstruturantesJá sabemos que podemos considerar objetos representados em imagens como conjuntos e que podemos também realizar operações sobre estes conjuntos. Bom, mas se vamos fazer operações sobre este conjunto de pixels, precisamos de um outro conjunto para realizar estas operações, não é mesmo? Muito bem! Para realizarmos estas operações precisamos dos Elementos estruturantes!Observe a Figura 2, na qual temos uma imagem representando um objeto em tons de cinza que é imóvel, enquanto que, se movendo, temos um pequeno conjunto de pixels que está percorrendo esta imagem (algo parecido com um filtro). Figura 2: Erosão A Figura 2 mostra uma operação entre dois conjuntos: O elemento estruturante (filtro que se move ao longo da imagem) e o objeto representado na imagem. O resultado desta operação é uma redução da quantidade de pixels que presentam o objeto. Esta operação é chamada de Erosão.Como funciona a Erosão em uma imagem?Vamos considerar que o conjunto A é o objeto representado na imagem e que o conjunto B é o elemento estruturante, ambos apresentados na Figura 2. Primeiro é feita uma “varredura” do conjunto B (Elemento estruturante) em A para que a origem de B passe por todos os elementos de A. Depois é feita uma verificação: Para cada localização da origem de B, considera-se que o pixel de A é um membro do novo conjunto caso todos os elementos de B que são diferentes de zero estejam contidos em A, caso contrário, descarta-se este pixel. (No exemplo da imagem, a cor cinza representa o valor 1 e a cor branca representa o valor zero). Depois de fazer essa varedura em toda a imagem, o resultado final é alcançado com o objeto tendo um conjunto menor de pixels conforme mostra a Figura 2. Outra Operação: A Dilatação Figura 3: Dilatação Observe na Figura 3 o procedimento que está sendo representado: Temos o conjunto A (o objeto representado na imagem), o conjunto B (elemento estruturante) e novamente realizamos uma varredura de B em A. A diferença é que agora, ao contrário do processo de Erosão, o objeto representado na imagem não “perdeu” pixels, isto é, ao invés de diminuirmos o conjunto A, ele ficou ainda maior. Quando isto acontece dizemos que o conjunto A sofreu Dilatação.O que aconteceu neste caso é que modificamos a operação a ser realizada entre estes dois conjuntos para aplicar a Dilatação, assim podemos dizer que a dilatação é aplicada seguindo os seguintes passos: Faz-se uma “varredura” do conjunto B (Elemento estruturante) em A para que a origem de B passe por todos os elementos de A. Depois é feita uma verificação: Para cada localização da origem de B, considera-se que o pixel de A é um membro do novo conjunto se pelo menos um elemento de B esteja contido em A, caso contrário, descarta-se este pixel. Perceba que antes mesmo que a origem do elemento estruturante esteja contida dentro do conjunto A, o pixel abaixo dela já está contida dentro de A, portanto considera-se que a posição da origem do elemento estruturante é parte do novo conjunto A. Depois de fazer essa varedura em toda a imagem, o resultado final é alcançado com o objeto tendo um conjunto maior de pixels conforme mostra a Figura 3. Mão na massa com a OpenCV e PythonPré-requisitosO que será necessário para realizar os tutoriais a seguir: Python 3.x instalado em sua máquina; OpenCV 4; Um editor de código ou IDE de sua preferência (Eu utilizo o VS Code); Os dados que utilizaremos para executar os tutoriais podem ser baixados aqui.Exemplo 1: ErosãoObserve a imagem abaixo. Note que não há muitos detalhes nesta imagem e isso pode facilitar as coisas para nós. Figura 4: Exemplo 1 Primeiro vamos supor que, por algum motivo, queremos eliminar a linha que conecta os circulos. Esta é uma operação de remoção de alguns pixels, por isso usaremos a Erosão para remover os pixels desta linha.Com a OpenCV o processo de aplicar a erosão é muito simples:import cv2, sysimport numpy as np#lendo a imagemimg = cv2.imread(&quot;ex1.png&quot;,0)if img is None: print(&quot;Não foi possível ler a imagem!&quot;) sys.exit()#criando um elemento estruturante de tamanho 5x5kernel = np.ones((5,5),np.uint8)#Aplicando a erosãoerosao = cv2.erode(img, kernel, iterations = 1)cv2.imwrite(&quot;erosao_ex1.jpg&quot;, erosao)No código acima utilizamos um elemento estruturante (que também pode ser chamado de kernel) de tamanho 5x5 para aplicar a erosão. Neste kernel, todos os valores são iguais a 1. Você pode criar kernels personalizados que tenham também valores igual a zero, mas aqui utilizamos desta forma para facilitar.O parâmetro iterations é o número de vezes que a erosão será aplicada à imagem. No nosso caso, aplicamos a erosão apenas uma vez.E temos o seguinte resultado: Figura 5: Resultado da Erosão da Fig. 4 Sinta-se à vontade para brincar um pouco com os parâmetros deste trecho de código! Veja o que acontece quando o kernel tem tamanhos menores, tamanhos maiores e quando o número de iterações é maior!Podemos dizer que as principais aplicações para a erosão são: Remoção de ruídos na imagem; Remoção de atributos que não são interessantes para a aplicação em questão. Podemos imaginar que os círculos brancos são topos de postes e que a linha era um fio passando por eles. Através da erosão removemos o fio. Exemplo 2: DilataçãoVamos supor agora que temos uma imagem na qual os círculos tem alguns “buracos” e queremos fechá-los. Precisamos de uma operação que possa adiiconar pixels aos circulos brancos, logo, precisamos da Dilatação. Figura 6: Exemplo 2 E novamente podemos usar a openCV para implementar facilmente esta operação:import cv2, sysimport numpy as npimg = cv2.imread(&quot;ex2.png&quot;,0)if img is None: print(&quot;Não foi possível ler a imagem!&quot;) sys.exit()kernel = np.ones((5,5),np.uint8)dilatacao = cv2.dilate(img,kernel,iterations = 2)cv2.imwrite(&quot;dilatacao_ex2.jpg&quot;, dilatacao)Neste exemplo usamos o mesmo kernel que utilizamos no exemplo anterior e aplicamos a dilatação duas vezes (iterations = 2) para fechar os círculos brancos.Temos o seguinte resultado: Figura 7: Resultado da Dilatação da Fig. 6Te convido novamente a realizar testes alterando o tamanho do kernel e a quantidade de iterações para ver o que acontece em cada caso!Quero novamente chamar a sua atenção para o fato de que nós não apenas “fechamos os círculos”, mas note que os círculos estão maiores do que na imagem original (Fig. 4) e estão assumindo uma forma um pouco retangular, por isso devemos usar a dilatação com cuidado para que não modifiquemos demais as características de um objeto na imagem!Podemos dizer que as principais aplicações da Dilatação são: Quando queremos que objetos sejam “destacados” fazendo com que eles fiquem maiores na imagem; Quando queremos “fechar buracos” ou até mesmo conectar elementos que estão muito próximos um do outro na imagem, porém não o suficiente para se conectarem. AtenciosamenteNatália C. de AmorimMestre em Ciências Geodésicas e Doutoranda em Ciências Geodésicas na Universidade Federal do Paraná." }, { "title": "Detecção de Bordas", "url": "/posts/cap1/", "categories": "bordas, canny", "tags": "detector de bordas, python, opencv", "date": "2020-12-04 08:25:34 -0300", "snippet": "Detecção de BordasA visão computacional é uma área da ciência que desenvolve teorias e tecnologias como objetivos de extrair informações de dados multidimensionais. Quase sempre, recorremos a uma analogia de como nós detectamos e reconhecemos objetos. Um objeto é caracterizado por conjuntos de atributos como: cor, texturas e forma geométrica. Nesse sentido, a extração de contorno poder representar informações importantes sobre um determinado objeto. Por exemplo, podemos identificar diversas formas geométricas como retângulo circulo, triângulos, linhas e outros. Além do que os médoto de detecção não utilizam muito recurso computacional sendo uma técnica atraente para aplicação em sistemas embaracados.Nessa capítulo vamos conhecer a base dos algoritmos de detecção de borda e aplicar o algoritmo de Canny.DependênciasPara executar os scripts mostrado aqui, você precisará ter em sua máquina uma versão do python 3 e o OpenCV instalados. python3 OpenCVO que é uma borda?Uma borda á caracteriza por uma variação abrupta entre os pixels vizinhos de uma imagem. Figura 1: Pista de corrida Primeiro vamos analizar apenas na linha selecionada Figura 1, podemos representa-la por uma função I(x) cujo domínio é uma lista [254,254,173,138,79,44,45,53]Como nossa função é discreta (só admite valor inteiro) não podemos calcular diretamente a derivada dessa função mais podemos fazer uma boa aproximação.A derivada é uma operação matemática que permite calcular a taxa de variação de uma função ou de dois pontos muito próximos. Ela é definida pela equação 1. Equção 1: Derivada. Supondo que x seja a posição que estamos na lista, então f(x) é o valor do pixel e f(x+h) é próximo pixel. Acontece que, quando o intervalo h for muito pequeno vamos pegar variações decorreste de ruídos na imagem. sendo assim, não vamos preocupar em fazer pequenos ajustes nesse sentido. Por exemplo, podemo dizer que nossa derivada no ponto x é dada por f(x+h)-f(x-h), ou seja, a diferença do próximo pixel pelo pixel anterior ao ponto x. Equação 2: Derivada aproximada. Desconsideramos a divisão por h da Equação 1, porque nesse contexto ele é apenas um normalizador da função, ou seja, ele será um parâmetro que vamos passar ao realizar os cálculos. &amp;lt;Figura 2: Derivada aproximada para 1. Um dos motivos da aproximação de fizemos é por conta dos ruídos, porém essa nova equação pode ser representada por um kernel. Computacionalmente é mais interessante convolver um kernel por uma imagem do que aplicar uma função. Figura 3: Kernel para calculo de derivada. Na Figua 4 realizamos essa operação para toda a linha da imagem 1, tente identificar onde está a região que selecionamos. Figura 4: Grafico de linha da selecionada na figura 1. A lista começa com valor alto, 254 decai ate 44 e sobe novamente para 53. Essa variação acontece no intervalo 160 à 178 (aproximado) do eixo x.Se expandirmos esse ideia para um plano 2D nossa função anterior pode ser descrita da seguinte forma. Figura 5: Aproximação de derivada. Agora temos derivadas parciais. Da mesma forma, podemos rescrever isso por um kernel. Figura 6: Kernel para derivada parcial. Esse par de kernel na Figura 6 tem o nome de operador Sobel. O OpenCV tem esse operador implementado aqui cv2.sobel, então vamos usar.import cv2ddepth = cv2.CV_16S# Carrega imagen &quot;frame.png&quot; em escala de cinzagray = cv2.imread(&quot;frame.png&quot;,cv2.IMREAD_GRAYSCALE)# calcula derivada de primeira ordem na direção x grad_x = cv2.Sobel(gray, ddepth, 1, 0, ksize=3,scale = 1)# calcula derivada de primieira ordem na direção ygrad_y = cv2.Sobel(gray, ddepth, 0, 1, ksize=3,scale = 1)# calcula valor absoluto e converte para uint8 abs_grad_x = cv2.convertScaleAbs(grad_x)abs_grad_y = cv2.convertScaleAbs(grad_y)# Calcula gradientegrad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)# concatena image gerada com a originalsaida=cv2.hconcat((grad,gray))# redimenciona em 60%saida=cv2.resize(saida,None,None,0.4,0.4)#salva imagemcv2.imwrite(&quot;saida.png&quot;,saida)cv2.imshow(&quot;janela&quot;, saida)cv2.waitKey(0) Figura 7: Resultado do Sobel.Perceba que aplicamos o operador Sobel duas vezes, primeiro na direção x e depois na direção y. A composição dessas derivadas é matematicamente conhecida como gradiente. O gradiente é um vetor que aponta na direção onde a função tem a maior variação. No entanto, o que nos interessa aqui é magnitude desse gradiente, ou seja, o quão abrupta é essa variação. O módulo do gradiente poder ser calulado usando a Equação 2 (calculamos com a função cv2.addWeighted). Equação 3: Magnitude do gradiente. O Sobel é uma das operações mais relevantes para detectar contorno em imagens. Embora exista alternativas como cv2.Scharr que tem uma aproximação melhor da derivada. O Sobel ainda é um dos principais métodos empregados nos algoritmos para detecção de borda.Algoritmo de CannyO algoritmo de Canny executa vários estágio para detectar uma borda.1. remoção de ruídos.Na Figura 4, o gráfico da derivada apresenta bastante ruido, isso acontece porque pegamos micros variações locais. Canny usa um filtro gaussiano para resolver isso. Veja como o filtro afeta a derivada na Figura 8. Figura 8: Efeito de filtro gaussiano.2. Calcular gradientes.O filtro Sobel discutido no tópico anterior é usado aqui para calcular os gradientes.3. Máximos locais.Nessa etapa uma varredura completa é realizada na imagem em busca de gradientes máximos locais, esse processo elemina bordas largas ou duplicadas.4. Limiar de hesterese.Tudo que esta abaixo de minVal é descartado. o que esta entre minVal e maxVal é mantido apenas se parte do contorno estiver acima de maxVal. Na Figura 9, A é mantido porque esta acima de maxVal, C é mantido, embora esteja abaixo de maxVal ele esta conectado a A. Já o B é removido, pois esta totalmente dentro da área delimitada. Figura 9: Região delimitada pelos liminar maxVal e minVal.Fonte: https://docs.opencv.org/master/da/d22/tutorial_py_canny.htmlUsando CannyNo OpenCV temos uma implementação do algoritmo de Canny, o segundo e o terceiro parâmetros passados, são minVal e maxVal.import cv2# Carrega imagem em escala de cinzagray = cv2.imread(&#39;frame.png&#39;,cv2.IMREAD_GRAYSCALE)# aplica algoritmo de Canny com minVal=100 e maxVal=200edges = cv2.Canny(gray,100,200)# concatena imagem original com o resultadosaida=cv2.hconcat((gray,edges))#redimenciona saida=cv2.resize(saida,None,None,0.4,0.4)#Mostra saidacv2.imshow(&quot;janela&quot;,saida)cv2.waitKey() Figura 9: Resultado do Canny. Aqui deixo um vídeo com animação gráfica do que discutimos nesse artigo.ConclusãoDe fato, a área de visão computacional é permeada por aplicações matemáticas de alta complexidade. No entanto, bibliotecas como OpenCV tem simplificado, permitindo que pessoas de diversas áreas desenvolva suas própias aplicações. Se você gostou desse assunto, junte-se a nós no grupo opencvBrasil .AtenciosamenteElton fernandes dos SantosEngenheiro eletricista pela UNEMAT e mestrando em Zootecnia na Universidade Federal do Mato Grosso UFMT.Autor do blog visioncompyReferências Documentação oficial OpenCV v 4.5.0: Sobel. fonte https://docs.opencv.org/master/d2/d2c/tutorial_sobel_derivatives.html Documentação oficial OpenCV v 4.5.0: Algoritmo de Canny. fonte https://docs.opencv.org/master/da/d22/tutorial_py_canny.html " } ]
